{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7337ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import glob\n",
    "import spacy\n",
    "import plotly.express as px\n",
    "\n",
    "#load core spanish library\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70dddd8",
   "metadata": {},
   "source": [
    "## Listo los documentos que conforman mi test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e23674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargo test_set_files\n",
    "mypath = '/Users/data/Documents/actas/'\n",
    "\n",
    "with open(mypath + 'test_set_files.txt') as json_file:\n",
    "    test_set_files = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4defeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veo cuáles son los documentos únicos\n",
    "test_files_list = list(test_set_files.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9abdfaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12264', '12863', '13874', '13881', '16560', '17174', '17513', '22264']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files_list = [y for x in test_files_list for y in x]\n",
    "\n",
    "# me quedo sólo con el doc id\n",
    "test_files_list = [t.split('.', 1)[0] for t in test_files_list]\n",
    "test_files_list = [t.split('-', 1)[0] for t in test_files_list]\n",
    "\n",
    "# me quedo con los únicos\n",
    "test_files_list = list(set(test_files_list))\n",
    "\n",
    "test_files_list.sort()\n",
    "test_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795e78c",
   "metadata": {},
   "source": [
    "## Listo los documentos que conforman el train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e52df37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20831',\n",
       " '12863',\n",
       " '14443',\n",
       " '26120',\n",
       " '25207',\n",
       " '12192',\n",
       " '17403',\n",
       " '26228',\n",
       " '12264',\n",
       " '17513',\n",
       " '20516',\n",
       " '22307',\n",
       " '14025',\n",
       " '13073',\n",
       " '23865',\n",
       " '20733',\n",
       " '10227',\n",
       " '10900',\n",
       " '22217',\n",
       " '20878',\n",
       " '22264',\n",
       " '12853',\n",
       " '25301',\n",
       " '17200',\n",
       " '13061',\n",
       " '17202',\n",
       " '25424',\n",
       " '23897',\n",
       " '24781',\n",
       " '22010',\n",
       " '17174',\n",
       " '26093',\n",
       " '13697',\n",
       " '10329',\n",
       " '23820',\n",
       " '16560',\n",
       " '12809']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargo train_set_files\n",
    "mypath = '/Users/data/Documents/actas/'\n",
    "\n",
    "with open(mypath + 'train_set_files.txt') as json_file:\n",
    "    train_set_files = json.load(json_file)\n",
    "    \n",
    "train_set_files = list(train_set_files.values())\n",
    "\n",
    "train_set_files = [y for x in train_set_files for y in x]\n",
    "\n",
    "# me quedo sólo con el doc id\n",
    "train_set_files = [t.split('.', 1)[0] for t in train_set_files]\n",
    "train_set_files = [t.split('-', 1)[0] for t in train_set_files]\n",
    "\n",
    "# me quedo con los únicos\n",
    "train_set_files = list(set(train_set_files))\n",
    "\n",
    "#train_set_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48e5be",
   "metadata": {},
   "source": [
    "## Veo si por casualidad hubo overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14c8f147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_set = set(train_set_files).intersection(set(test_set_files))\n",
    "intersection_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823ab52",
   "metadata": {},
   "source": [
    "## Los documentos del test set los corro enteros\n",
    "Empiezo con un documento solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77710032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto lo tengo que dejar en un módulo!\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32,\n",
    "                              np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52ead785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['distiluse-base-multilingual-cased-v1',\n",
    "            'stsb-xlm-r-multilingual',\n",
    "            'quora-distilbert-multilingual',\n",
    "            'paraphrase-xlm-r-multilingual-v1',\n",
    "            'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "            'paraphrase-multilingual-mpnet-base-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c3df662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 of 6  ( distiluse-base-multilingual-cased-v1 )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n",
      "model 1 of 6  ( stsb-xlm-r-multilingual )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n",
      "model 2 of 6  ( quora-distilbert-multilingual )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n",
      "model 3 of 6  ( paraphrase-xlm-r-multilingual-v1 )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n",
      "model 4 of 6  ( paraphrase-multilingual-MiniLM-L12-v2 )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n",
      "model 5 of 6  ( paraphrase-multilingual-mpnet-base-v2 )\n",
      "\tfile 0 of 8\n",
      "\tfile 1 of 8\n",
      "\tfile 2 of 8\n",
      "\tfile 3 of 8\n",
      "\tfile 4 of 8\n",
      "\tfile 5 of 8\n",
      "\tfile 6 of 8\n",
      "\tfile 7 of 8\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for m in model_list:\n",
    "    print('model', j, 'of', len(model_list), ' (', m, ')')\n",
    "   # m = 'distiluse-base-multilingual-cased-v1'\n",
    "    model = SentenceTransformer(m)\n",
    "    model.max_seq_length = 512\n",
    "    \n",
    "    i=0\n",
    "    for doc_id in test_files_list:\n",
    "        #doc_id = test_files_list[0]\n",
    "        print('\\tfile', i, 'of', len(test_files_list))\n",
    "\n",
    "        # cargo todas las páginas de ese documento\n",
    "        file_list = glob.glob(mypath + \"ocr/\" + doc_id + '/*')\n",
    "        file_list.sort()\n",
    "\n",
    "        # a cada una le calculo el embedding\n",
    "\n",
    "        # este diccionario tiene, para cada página, su embedding\n",
    "        embedding_dict = {}\n",
    "\n",
    "        for f in file_list:\n",
    "\n",
    "            # me quedo con el identificador de la página\n",
    "            file_id = f.split('/')[-1].split('.')[0]\n",
    "            #print(file_id)\n",
    "\n",
    "            # estaría bueno encapsular esto en una función\n",
    "            with open(f) as json_file:\n",
    "                pagina = json.load(json_file)\n",
    "\n",
    "            # armo la pagina\n",
    "            full_page = []\n",
    "            for b in pagina['Blocks']:\n",
    "                if b['BlockType'] == 'LINE':\n",
    "                    full_page.append(b['Text'])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            full_page = ' '.join(full_page)\n",
    "\n",
    "            doc = nlp(full_page)\n",
    "\n",
    "            # me quedo con el primer 0.5 de las oraciones\n",
    "            sentences_list = list(doc.sents)\n",
    "            n_keep = int(0.5*len(sentences_list))\n",
    "            text = ''.join(str(s) for s in sentences_list[:n_keep])\n",
    "\n",
    "            # calculo el embedding\n",
    "            embedding = model.encode(str(text))\n",
    "\n",
    "            embedding_dict[file_id] = embedding\n",
    "\n",
    "        # exporto\n",
    "        dumped = json.dumps(embedding_dict, cls=NumpyEncoder)\n",
    "        with open(mypath + 'embedding_results/test_full_doc/embeddings_' + doc_id + '_' + m + '.txt', 'w') as file:\n",
    "            file.write(dumped)\n",
    "        i=i+1\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283636b3",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "Empiezo con un solo documento y un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e7688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
